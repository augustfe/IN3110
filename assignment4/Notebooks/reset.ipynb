{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.policy import default\n",
    "from typing import List  # isort:skip\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "from requesting_urls import get_html\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "def better_articles(html):\n",
    "  base_url = \"https://en.wikipedia.org\"\n",
    "  articles = set()\n",
    "  long_pat = re.compile(r\"(?<=href=\\\")((http\\:\\/\\/en\\.wikipedia[^#\\\\\\\"]+)|(\\/wiki[^#\\\"]+))\")\n",
    "  \n",
    "  for article in long_pat.findall(html):\n",
    "    if article is not None:\n",
    "      if (article[0][0] == \"/\"):\n",
    "        article = base_url + article[0]\n",
    "      else:\n",
    "        article = article[0]\n",
    "      if not \":\" in article[6:]:\n",
    "        articles.add(article)\n",
    "  \n",
    "  return articles\n",
    "\n",
    "class Node:\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "    self.last = None\n",
    "\n",
    "class Graph:\n",
    "\n",
    "  def __init__(self, target):\n",
    "    self.graph = defaultdict(list)\n",
    "    self.trail = {}\n",
    "    self.target = target\n",
    "    self.count = 0\n",
    "    self.visited = defaultdict(lambda: False)\n",
    "\n",
    "  def extendGraph(self, node):\n",
    "    html: str = get_html(node)\n",
    "    articles = better_articles(html)\n",
    "    for article in articles:\n",
    "      if article != node:\n",
    "        self.addEdge(node, article)\n",
    "\n",
    "  def addEdge(self, u, v):\n",
    "    self.graph[u].append(v)\n",
    "    if not v in self.trail:\n",
    "      node = Node(v)\n",
    "      self.trail[v] = node\n",
    "      node.last = self.trail[u]\n",
    "\n",
    "  def move_through_queue(self, url):\n",
    "    self.queue.remove(url)\n",
    "    self.extendGraph(url)\n",
    "    self.count += 1\n",
    "    print(self.count, url)\n",
    "    path = []\n",
    "    find = False\n",
    "    for i in self.graph[url]:\n",
    "      if i == self.target:\n",
    "        print(\"Found it here!\")\n",
    "        path = [i]\n",
    "        breadcrumbs = self.trail[i]\n",
    "        while breadcrumbs.last != None:\n",
    "          path.insert(0, breadcrumbs.last.data)\n",
    "          breadcrumbs = breadcrumbs.last\n",
    "        print(path)\n",
    "        find = True\n",
    "      if self.visited[i] == False:\n",
    "        self.queue.append(i)\n",
    "        self.visited[i] = True\n",
    "    return url, path, find\n",
    "\n",
    "  def Search(self, start):\n",
    "    self.trail[start] = Node(start)\n",
    "    \n",
    "    self.queue = []\n",
    "    count = 0\n",
    "\n",
    "    self.queue.append(start)\n",
    "    self.visited[start] = True\n",
    "\n",
    "    while True:\n",
    "      with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:\n",
    "        futures = [executor.submit(self.move_through_queue, url) for url in self.queue]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "          url, path, find = future.result()\n",
    "          print(count, url)\n",
    "          if find:\n",
    "            return path\n",
    "        #     node = queue.pop(0)\n",
    "        #     self.extendGraph(node)\n",
    "        #     count += 1\n",
    "        #     print(count, node)\n",
    "        #     for i in self.graph[node]:\n",
    "              \n",
    "        #       if i == self.target:\n",
    "        #         print(\"Found it here!\")\n",
    "        #         path = [i]\n",
    "        #         breadcrumbs = self.trail[i]\n",
    "        #         while breadcrumbs.last != None:\n",
    "        #           path.insert(0, breadcrumbs.last.data)\n",
    "        #           breadcrumbs = breadcrumbs.last\n",
    "\n",
    "        #         print(path)\n",
    "        #         return path\n",
    "        #       if visited[i] == False:\n",
    "        #         queue.append(i)\n",
    "        #         visited[i] = True\n",
    "      \n",
    "          \n",
    "def find_path(start: str, finish: str) -> List[str]:\n",
    "    \"\"\"Find the shortest path from `start` to `finish`\n",
    "\n",
    "    Arguments:\n",
    "      start (str): wikipedia article URL to start from\n",
    "      finish (str): wikipedia article URL to stop at\n",
    "\n",
    "    Returns:\n",
    "      urls (list[str]):\n",
    "        List of URLs representing the path from `start` to `finish`.\n",
    "        The first item should be `start`.\n",
    "        The last item should be `finish`.\n",
    "        All items of the list should be URLs for wikipedia articles.\n",
    "        Each article should have a direct link to the next article in the list.\n",
    "    \"\"\"\n",
    "    graph = Graph(finish)\n",
    "    path = graph.Search(start)\n",
    "    assert path[0] == start\n",
    "    assert path[-1] == finish\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"https://en.wikipedia.org/wiki/The_Emoji_Movie\"\n",
    "finish = \"https://en.wikipedia.org/wiki/Vietnam_War\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from typing import IO\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stderr,\n",
    ")\n",
    "logger = logging.getLogger(\"areq\")\n",
    "logging.getLogger(\"chardet.charsetprober\").disabled = True\n",
    "async def fetch_html(url: str, session: ClientSession, **kwargs) -> str:\n",
    "    \"\"\"GET request wrapper to fetch page HTML.\n",
    "\n",
    "    kwargs are passed to `session.request()`.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = await session.request(method=\"GET\", url=url, **kwargs)\n",
    "    resp.raise_for_status()\n",
    "    logger.info(\"Got response [%s] for URL: %s\", resp.status, url)\n",
    "    html = await resp.text()\n",
    "    return html\n",
    "\n",
    "async def parse(url: str, session: ClientSession, **kwargs) -> set:\n",
    "    \"\"\"Find HREFs in the HTML of `url`.\"\"\"\n",
    "    found = set()\n",
    "    try:\n",
    "        html = await fetch_html(url=url, session=session, **kwargs)\n",
    "    except (\n",
    "        aiohttp.ClientError,\n",
    "        aiohttp.http_exceptions.HttpProcessingError,\n",
    "    ) as e:\n",
    "        logger.error(\n",
    "            \"aiohttp exception for %s [%s]: %s\",\n",
    "            url,\n",
    "            getattr(e, \"status\", None),\n",
    "            getattr(e, \"message\", None),\n",
    "        )\n",
    "        return found\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Non-aiohttp exception occured:  %s\", getattr(e, \"__dict__\", {})\n",
    "        )\n",
    "        return found\n",
    "    else:\n",
    "        base_url = \"https://en.wikipedia.org\"\n",
    "        long_pat = re.compile(r\"(?<=href=\\\")((http\\:\\/\\/en\\.wikipedia[^#\\\\\\\"]+)|(\\/wiki[^#\\\"]+))\")\n",
    "        \n",
    "        for article in long_pat.findall(html):\n",
    "            if article is not None:\n",
    "                if (article[0][0] == \"/\"):\n",
    "                    article = base_url + article[0]\n",
    "                else:\n",
    "                    article = article[0]\n",
    "                if not \":\" in article[6:]:\n",
    "                    found.add(article)\n",
    "        return found\n",
    "\n",
    "async def bulk_crawl_and_write(file: IO, urls: set, **kwargs) -> None:\n",
    "    \"\"\"Crawl & write concurrently to `file` for multiple `urls`.\"\"\"\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            tasks.append(\n",
    "                write_one(file=file, url=url, session=session, **kwargs)\n",
    "            )\n",
    "        await asyncio.gather(*tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'Graph.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nAttributeError: Can't pickle local object 'Graph.__init__.<locals>.<lambda>'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfind_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [16], line 132\u001b[0m, in \u001b[0;36mfind_path\u001b[0;34m(start, finish)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"Find the shortest path from `start` to `finish`\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Each article should have a direct link to the next article in the list.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m graph \u001b[38;5;241m=\u001b[39m Graph(finish)\n\u001b[0;32m--> 132\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m path[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m start\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m finish\n",
      "Cell \u001b[0;32mIn [16], line 91\u001b[0m, in \u001b[0;36mGraph.Search\u001b[0;34m(self, start)\u001b[0m\n\u001b[1;32m     89\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_through_queue, url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[0;32m---> 91\u001b[0m   url, path, find \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28mprint\u001b[39m(count, url)\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m find:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py:244\u001b[0m, in \u001b[0;36mQueue._feed\u001b[0;34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m obj \u001b[39m=\u001b[39m _ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj)\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m wacquire \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     send_bytes(obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'Graph.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "find_path(start, finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/augustfemtehjell/Documents/Høst22/IN3110-augustfe/assignment4/reset.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augustfemtehjell/Documents/H%C3%B8st22/IN3110-augustfe/assignment4/reset.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m3 workers slept in parallel for \u001b[39m\u001b[39m{\u001b[39;00mtotal_slept_for\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augustfemtehjell/Documents/H%C3%B8st22/IN3110-augustfe/assignment4/reset.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtotal expected sleep time: \u001b[39m\u001b[39m{\u001b[39;00mtotal_sleep_time\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/augustfemtehjell/Documents/H%C3%B8st22/IN3110-augustfe/assignment4/reset.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(main())\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "async def worker(name, queue):\n",
    "    while True:\n",
    "        # Get a \"work item\" out of the queue.\n",
    "        sleep_for = await queue.get()\n",
    "\n",
    "        # Sleep for the \"sleep_for\" seconds.\n",
    "        await asyncio.sleep(sleep_for)\n",
    "\n",
    "        # Notify the queue that the \"work item\" has been processed.\n",
    "        queue.task_done()\n",
    "\n",
    "        print(f'{name} has slept for {sleep_for:.2f} seconds')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Create a queue that we will use to store our \"workload\".\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    # Generate random timings and put them into the queue.\n",
    "    total_sleep_time = 0\n",
    "    for _ in range(20):\n",
    "        sleep_for = random.uniform(0.05, 1.0)\n",
    "        total_sleep_time += sleep_for\n",
    "        queue.put_nowait(sleep_for)\n",
    "\n",
    "    # Create three worker tasks to process the queue concurrently.\n",
    "    tasks = []\n",
    "    for i in range(3):\n",
    "        task = asyncio.create_task(worker(f'worker-{i}', queue))\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Wait until the queue is fully processed.\n",
    "    started_at = time.monotonic()\n",
    "    await queue.join()\n",
    "    total_slept_for = time.monotonic() - started_at\n",
    "\n",
    "    # Cancel our worker tasks.\n",
    "    for task in tasks:\n",
    "        task.cancel()\n",
    "    # Wait until all worker tasks are cancelled.\n",
    "    await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    print('====')\n",
    "    print(f'3 workers slept in parallel for {total_slept_for:.2f} seconds')\n",
    "    print(f'total expected sleep time: {total_sleep_time:.2f} seconds')\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ea1c92bb7bd6f68c1498ed59ea13162eb4a59740e46202fcf3dc269798747a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
